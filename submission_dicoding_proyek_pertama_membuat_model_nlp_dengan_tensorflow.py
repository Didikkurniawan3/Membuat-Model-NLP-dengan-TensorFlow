# -*- coding: utf-8 -*-
"""Submission Dicoding Proyek Pertama : Membuat Model NLP dengan TensorFlow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ne1UcvVk0mx_aQ1z4IrDhRm-N4cJvEoI

**Submission Dicoding Proyek Pertama : Membuat Model NLP dengan TensorFlow**

Nama : Didik Kurniawan

Username Dicoding : didik_kurniawan_slVA

Email : didik2584@gmail.com
"""

import pandas as pd

from google.colab import files
uploaded = files.upload()

df_tweet = pd.read_csv('/content/covid19_tweet160621.csv')
df_tweet = df_tweet[['sentiment', 'Content']]
df_tweet

def f(row):
  if row['sentiment'] > 0:
    val = 'positive'
  elif row['sentiment'] < 0:
    val = 'negative'
  else:
    val = 'neutral'
  return val

df_tweet['sentimen'] = df_tweet.apply(f, axis=1)

pd.value_counts(df_tweet['sentimen']).plot.bar()

df_tweet = df_tweet.drop(columns=['sentiment'])

category  = pd.get_dummies(df_tweet['sentimen'])

new_df = pd.concat([df_tweet, category], axis=1)
new_df = new_df.drop(columns=['sentimen'])
new_df

print(new_df.shape)
new_df['Content'] = new_df['Content'].str.lower().str.replace(r"[^a-zA-Z]", " ").str.replace(r"\s+[a-zA-Z]\s+", " ").str.strip().replace('\s+', ' ', regex=True).str.strip()

new_df.drop_duplicates(keep=False, inplace=True)
print(new_df.shape)
new_df

import nltk
from nltk.corpus import stopwords
nltk.download('stopwords')
def stopword(sentence):
    review = [words for words in sentence.split() if words not in set(stopwords.words('english'))]
    review = ' '.join(review)
    return review
print(stopwords.words('english'))
new_df['Content'] = new_df['Content'].apply(stopword)
new_df

stemmer = nltk.SnowballStemmer("english")

def stemm_text(text):
    text = ' '.join(stemmer.stem(word) for word in text.split(' '))
    return text
new_df['Content'] = new_df['Content'].apply(stemm_text)
new_df

sentimen = new_df['Content'].values
label = new_df[['negative', 'neutral', 'positive']].values

from sklearn.model_selection import train_test_split
sentimen_latih, sentimen_test, label_latih, label_test = train_test_split(sentimen, label, test_size=0.2)

import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(num_words=300000, oov_token='x')
tokenizer.fit_on_texts(sentimen_latih)

sekuens_latih = tokenizer.texts_to_sequences(sentimen_latih)
sekuens_test = tokenizer.texts_to_sequences(sentimen_test)

padded_latih = pad_sequences(sekuens_latih)
padded_test = pad_sequences(sekuens_test)

padded_latih.shape

a = tokenizer.word_index
jumlah_vocab = len(a)
jumlah_vocab

model = tf.keras.Sequential([
                             tf.keras.layers.Embedding(input_dim=jumlah_vocab+2, output_dim=16),
    tf.keras.layers.Dropout(0.2),
                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])
model.summary()

model.compile(
    optimizer='rmsprop',
    loss='categorical_crossentropy',
    metrics=['accuracy'])

from keras.callbacks import ModelCheckpoint
checkpoint1 = ModelCheckpoint("best_model1.hdf5", monitor='val_accuracy', verbose=1,save_best_only=True, mode='auto', period=1,save_weights_only=False)

history = model.fit(padded_latih, label_latih, epochs=30, validation_data=(padded_test, label_test), callbacks=[checkpoint1])

import matplotlib.pyplot as plt
plt.plot(history.history['loss'])
plt.title('Model loss')
plt.ylabel('loss')
plt.xlabel('Epoch')
plt.legend(['Train'], loc='upper right')
plt.show()

plt.plot(history.history['accuracy'])
plt.title('Model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('epoch')
plt.legend(['Train'], loc='lower right')
plt.show()